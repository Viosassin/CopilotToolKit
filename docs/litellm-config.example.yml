# Set OPENAI_BASE_URL to http://hurricane.server:4136/v1 or
#     https://llm.hurricane.home/v1 to access proxied models.
model_list:
  # ? Local Inference Models (via vLLM, Ollama, etc.)
  # ?   Usually hosted on Adam, Adam, or Hurricane servers.
  # ? Use the `openai/` prefix for vLLM or Ollama models.
  # ? For local deployment, we set higher TPM and RPM to prevent rate limiting.
  # ?   When offline, some of them are load balanced.
  # * ######################################################
  # *        Self-Hosted models (vLLM. Ollama, etc)        #
  # * ######################################################

  - model_name: "microsoft/phi-3-mini-128k-instruct"
    # All of phi-3-mini-128k-instruct models are Q3_K_M.
    # Reserves 21.5 GB of memory space.
    litellm_params:
      # Running on Hurricane
      api_base: http://ollama:11434/v1
      model: "openai/phi-3-mini-128k-instruct:latest"
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      stream: True
      tpm: 100000
      rpm: 100000
    model_info:
      metadata: "Hosted on the Hurricane (CPU Only)."
      max_tokens: 51200
      max_input_tokens: 51200
      max_output_tokens: 51200
      input_cost_per_token: 0.00001
      output_cost_per_token: 0.00001

  - model_name: "microsoft/phi-3-mini-128k-instruct"
    # Reserves 21.5 GB of memory space.
    litellm_params:
      # Running on Adam
      api_base: http://adam.server:11434/v1
      model: "openai/phi-3-mini-128k-instruct:latest"
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      stream: True
      tpm: 200000
      rpm: 200000
    model_info:
      metadata: "Hosted on the Adam (GPU)."
      max_tokens: 51200
      max_input_tokens: 51200
      max_output_tokens: 51200
      input_cost_per_token: 0.00001
      output_cost_per_token: 0.00001

  - model_name: "microsoft/phi-3-mini-128k-instruct@hurricane"
    # Reserves 21.5 GB of memory space.
    litellm_params:
      # Running on Hurricane
      api_base: http://ollama:11434/v1
      model: "openai/phi-3-mini-128k-instruct:latest"
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      stream: True
      tpm: 100000
      rpm: 100000
    model_info:
      metadata: "Hosted on the Hurricane (CPU)."
      max_tokens: 51200
      max_input_tokens: 51200
      max_output_tokens: 51200
      input_cost_per_token: 0.00001
      output_cost_per_token: 0.00001

  - model_name: "microsoft/phi-3-mini-128k-instruct@Adam"
    # Reserves 21.5 GB of memory space.
    litellm_params:
      # Running on Adam
      api_base: http://adam.server:11434/v1
      model: "openai/phi-3-mini-128k-instruct:latest"
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      stream: True
      tpm: 200000
      rpm: 200000
    model_info:
      metadata: "Hosted on the Adam (GPU)."
      max_tokens: 51200
      max_input_tokens: 51200
      max_output_tokens: 51200
      input_cost_per_token: 0.00001
      output_cost_per_token: 0.00001

  - model_name: "meta/llama3-70b-8192"
    litellm_params:
      api_base: http://adam.server:11434/v1
      model: "openai/llama3:70b-instruct-q4_K_M"
      api_key: "xx"
      stream: False
      tpm: 10000
      rpm: 1000
    model_info:
      metadata: "Hosted on the Adam."
      max_tokens: 8192
      max_input_tokens: 8192
      max_output_tokens: 8192
      input_cost_per_token: 0.000
      output_cost_per_token: 0.000

  - model_name: "meta/llama3-70b-8192"
    litellm_params:
      api_base: http://adam.server:61434/v1
      model: "openai//data/models/Meta-Llama-3-70B-Instruct" # set `openai/` to use the openai route
      api_key: "xx"
      temperature: 0.4
      stream: True
      tpm: 20000
      rpm: 2000
    model_info:
      metadata: "Hosted on the Adam Server."
      author: "https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO"
      max_tokens: 32768
      max_input_tokens: 32768
      max_output_tokens: 8192
      input_cost_per_token: 0.000
      output_cost_per_token: 0.000

  - model_name: "custom/nous-hermes-2-mixtral-8x7b-dpo"
    litellm_params:
      api_base: http://adam.server:61434/v1
      model: "openai//data/models/Nous-Hermes-2-Mixtral-8x7B-DPO" # set `openai/` to use the openai route
      api_key: "xx"
      temperature: 0.4
      stream: True
      tpm: 10000
      rpm: 1000
    model_info:
      metadata: "Hosted on the Adam Server."
      author: "https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO"
      max_tokens: 32768
      max_input_tokens: 32768
      max_output_tokens: 8192
      input_cost_per_token: 0.000
      output_cost_per_token: 0.000

  - model_name: "custom/WizardLM-2-8x22B-AWQ"
    litellm_params:
      api_base: http://adam.server:61434/v1
      model: "openai//data/models/WizardLM-2-8x22B-AWQ" # set `openai/` to use the openai route
      api_key: "xx"
      temperature: 0.4
      stream: False
      tpm: 10000
      rpm: 1000
    model_info:
      metadata: "Hosted on the Adam Server."
      author: "https://huggingface.co/microsoft/WizardLM-2-8x22B"
      max_tokens: 16384
      max_input_tokens: 16384
      max_output_tokens: 16384
      input_cost_per_token: 0.000
      output_cost_per_token: 0.000

  # Cloud Inference Models (OpenAI, Google, Anthropic, etc.)
  # * ##############################################
  # * #          Groq Cloud models                 #
  # * ##############################################

  - model_name: "meta/llama3-70b-8192"
    litellm_params:
      model: groq/llama3-70b-8192
      # api_base: https://api.groq.com/openai/v1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      stream: True
      tpm: 7000
      rpm: 30

  - model_name: "meta/llama3-8b-8192"
    litellm_params:
      model: groq/llama3-8b-8192
      # api_base: https://api.groq.com/openai/v1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.4
      stream: True
      tpm: 14400
      rpm: 30

  # * ##############################################
  # * #          Mistral AI models                 #
  # * ##############################################

  - model_name: "mistralai/mistral-tiny"
    litellm_params:
      model: mistral/mistral-tiny
      # api_base: https://api.mistral.ai/v1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.8
      stream: True
      # tpm: 9000
      # rpm: 30

  - model_name: "mistralai/mistral-small"
    litellm_params:
      model: mistral/mistral-small
      # api_base: https://api.mistral.ai/v1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.8
      stream: True
      # tpm: 9000
      # rpm: 30

  - model_name: "mistralai/mistral-medium"
    litellm_params:
      model: mistral/mistral-medium
      # api_base: https://api.mistral.ai/v1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.8
      stream: True
      # tpm: 9000
      # rpm: 30

  - model_name: "mistralai/mixtral-8x7b-32768"
    litellm_params:
      model: groq/mixtral-8x7b-32768
      # api_base: https://api.groq.com/openai/v1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.8
      stream: True
      tpm: 9000
      rpm: 30

  # * ##############################################
  # * #               OpenAI models                #
  # * ##############################################

  - model_name: "openai/gpt-4-turbo"
    litellm_params:
      # Currently points to gpt-4-turbo-2024-04-09.
      model: openai/gpt-4-turbo # set `openai/` to use the openai route
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.6
      stream: True

  - model_name: "openai/gpt-3.5-turbo"
    litellm_params:
      # Currently points to gpt-3.5-turbo-0125
      model: openai/gpt-3.5-turbo # set `openai/` to use the openai route
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.6
      stream: True

  # * ##############################################
  # * #           Vertex AI (GCP) models           #
  # * ##############################################


  # * ##############################################
  # * #            Google Studio models            #
  # * ##############################################

  - model_name: "google/gemini-pro"
    litellm_params:
      # Note: gemini-pro is an alias for gemini-1.0-pro.
      model: gemini/gemini-pro
      # Created with primary account
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.4
      stream: True

  - model_name: "google/gemini-1.5-pro"
    litellm_params:
      model: gemini/gemini-1.5-pro-latest
      # Created with primary account
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.4
      stream: True
      # Beta restrictions: 2 queries per minute, 1000 queries per day
      rpm: 2

  - model_name: "google/gemini-pro"
    litellm_params:
      # Note: gemini-pro is an alias for gemini-1.0-pro.
      model: gemini/gemini-pro
      # Created with secondary account #1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.4
      stream: True

  - model_name: "google/gemini-1.5-pro"
    litellm_params:
      model: gemini/gemini-1.5-pro-latest
      # Created with secondary account #1
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.4
      stream: True
      # Beta restrictions: 2 queries per minute, 1000 queries per day
      rpm: 2

  # * ##############################################
  # * #            Anthropic models                #
  # * ##############################################

  - model_name: "anthropic/claude-3-opus"
    litellm_params:
      model: claude-3-opus-20240229
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.6
      stream: False

  - model_name: "anthropic/claude-3-sonnet"
    litellm_params:
      model: claude-3-sonnet-20240229
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.6
      stream: False

  - model_name: "anthropic/claude-3-haiku"
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.4
      stream: False

  - model_name: "anthropic/claude-instant-1.2"
    litellm_params:
      model: claude-instant-1.2
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
      temperature: 0.4
      stream: False

  # * ##############################################
  # * #            Embedding models                #
  # * ##############################################

  - model_name: "openai/text-embedding-ada-002"
    litellm_params:
      # model name for litellm.embedding(model=text-embedding-ada-002)
      model: text-embedding-ada-002
      api_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"


# ! ##############################################
# ! #            General Settings                #
# ! ##############################################

litellm_settings:
  # module level litellm settings - https://github.com/BerriAI/litellm/blob/main/litellm/__init__.py
  num_retries: 3
  request_timeout: 60
  fallbacks:
    [
      {
        "custom/nous-hermes-2-mixtral-8x7b-dpo":
          ["mistralai/mixtral-8x7b-32768"],
      },
    ]
  allowed_fails: 3
  success_callback:
    - langfuse
  failure_callback:
    - langfuse
  drop_params: True
  cache: True # cache responses
  # cache_params are optional
  cache_params:
    type: "redis" # The type of cache to initialize. Can be "local" or "redis". Defaults to "local".
    host: "redis" # The host address for the Redis cache. Required if type is "redis".
    port: 6379 # The port number for the Redis cache. Required if type is "redis".
    password: "" # The password for the Redis cache. Required if type is "redis".
    supported_call_types:
      - acompletion
      - completion
      - embedding
      - aembedding
  upperbound_key_generate_params:
    duration: 30d
    max_budget: 100

  # ! Not working yet
  default_team_settings:
    - team_id: librechat
      success_callback:
        - langfuse
      failure_callback:
        - langfuse
      langfuse_host: http://langfuse-server:3000
      langfuse_public_key: pk-lf-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
      langfuse_secret: sk-lf-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

router_settings:
  routing_strategy: usage-based-routing-v2
  redis_host: redis
  redis_password: ""
  redis_port: 6379
  enable_pre_call_check: true
  allowed_fails: 3
  model_alias_map:
    {
      "gpt-4-turbo": "openai/gpt-4-turbo",
      "gpt-3.5-turbo": "openai/gpt-3.5-turbo",
      "text-embedding-ada-002": "openai/text-embedding-ada-002",
    }

general_settings:
  # [OPTIONAL] Only use this if you to require all calls to contain this key (Authorization: Bearer sk-1234)
  master_key: "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  # ui_access_mode: "admin_only"

environment_variables:
  # * Accessible via https://langfuse.hurricane.home/
  LANGFUSE_HOST: http://langfuse-server:3000
  LANGFUSE_PUBLIC_KEY: pk-lf-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  LANGFUSE_SECRET_KEY: sk-lf-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  UI_USERNAME: xx
  UI_PASSWORD: xx

model_alias_map:
  #- model_alias: litellm_model_name
  gpt-4-turbo: openai/gpt-4-turbo
  gpt-3.5-turbo: openai/gpt-3.5-turbo
  text-embedding-ada-002: openai/text-embedding-ada-002
